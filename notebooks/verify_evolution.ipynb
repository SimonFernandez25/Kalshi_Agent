{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Evolution Pipeline Verification\n",
                "\n",
                "Tests all evolution modules + live demo with real Kalshi event."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys, os\n",
                "from pathlib import Path\n",
                "\n",
                "# Set project root\n",
                "notebook_dir = Path(os.getcwd()).resolve()\n",
                "if notebook_dir.name == 'notebooks':\n",
                "    project_root = notebook_dir.parent\n",
                "else:\n",
                "    project_root = notebook_dir\n",
                "\n",
                "os.chdir(project_root)\n",
                "if str(project_root) not in sys.path:\n",
                "    sys.path.insert(0, str(project_root))\n",
                "\n",
                "print(f\"Project root: {project_root}\")\n",
                "print(f\"Python: {sys.executable}\")\n",
                "print(f\"sys.path[0]: {sys.path[0]}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test basic imports first\n",
                "import json\n",
                "import tempfile\n",
                "import shutil\n",
                "from datetime import datetime, timezone\n",
                "from textwrap import dedent\n",
                "\n",
                "# Root-level modules\n",
                "import schemas\n",
                "import config\n",
                "\n",
                "# Packages at root\n",
                "from tools.registry import ToolRegistry, build_default_registry\n",
                "from tools.base_tool import BaseTool\n",
                "from agent.graph import build_agent_graph, build_evolution_graph\n",
                "from api.kalshi_client import KalshiClient\n",
                "from engine.tool_runner import run_tools\n",
                "from engine.scorer import compute_score\n",
                "\n",
                "# Evolution package (nested under prediction_agent/)\n",
                "from prediction_agent.evolution.schemas import (\n",
                "    ExecutionLogEntry, GapReport, ToolSpec, VerificationResult,\n",
                "    ToolLifecycleRecord, RiskLevel, ToolStatus\n",
                ")\n",
                "from prediction_agent.evolution.execution_logger import log_execution\n",
                "from prediction_agent.evolution.tool_gap_analyzer import analyze_gaps\n",
                "from prediction_agent.evolution.tool_spec_generator import generate_tool_spec\n",
                "from prediction_agent.evolution.tool_builder import build_tool\n",
                "from prediction_agent.evolution.tool_verifier import verify_tool\n",
                "from prediction_agent.evolution.tool_lifecycle_manager import ToolLifecycleManager\n",
                "\n",
                "print(\"All imports successful!\")\n",
                "print(f\"ENABLE_EVOLUTION = {config.ENABLE_EVOLUTION}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create temp directory\n",
                "TEMP_DIR = Path(tempfile.mkdtemp(prefix=\"evo_verify_\"))\n",
                "print(f\"Temp dir: {TEMP_DIR}\")\n",
                "\n",
                "def check(label, condition):\n",
                "    status = \"PASS\" if condition else \"FAIL\"\n",
                "    print(f\"[{status}] {label}\")\n",
                "    if not condition:\n",
                "        raise AssertionError(label)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Quick Validation Tests"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test schema\n",
                "spec = ToolSpec(tool_name=\"test\", description=\"Test\", deterministic=True, risk_level=\"low\")\n",
                "check(\"ToolSpec creation\", spec.tool_name == \"test\")\n",
                "\n",
                "# Test gap analyzer\n",
                "gap_file = TEMP_DIR / \"gap.jsonl\"\n",
                "with open(gap_file, \"w\") as f:\n",
                "    for i in range(3):\n",
                "        f.write(json.dumps({\n",
                "            \"run_id\": f\"r-{i}\", \"market_id\": \"M\", \"market_title\": \"T\",\n",
                "            \"selected_tools\": [\"t1\"], \"tool_weights\": [1.0],\n",
                "            \"tool_outputs\": [{\"tool_name\": \"t1\", \"output_vector\": [0.5]}],\n",
                "            \"final_score\": 0.5, \"threshold\": 0.5, \"bet_triggered\": True,\n",
                "            \"reasoning_segments\": \"test\", \"failed_tool_attempts\": [],\n",
                "            \"total_tokens_used\": 100, \"timestamp\": \"2025-01-01T00:00:00+00:00\"\n",
                "        }) + \"\\n\")\n",
                "result = analyze_gaps(log_path=gap_file, min_runs=5)\n",
                "check(\"Gap analyzer: insufficient data -> None\", result is None)\n",
                "\n",
                "# Test verifier\n",
                "safe_code = dedent('''\n",
                "from tools.base_tool import BaseTool\n",
                "from schemas import EventInput, ToolOutput\n",
                "\n",
                "class TestTool(BaseTool):\n",
                "    @property\n",
                "    def name(self): return \"test_tool\"\n",
                "    @property\n",
                "    def description(self): return \"Test\"\n",
                "    def run(self, event, **kwargs):\n",
                "        return ToolOutput(tool_name=self.name, output_vector=[0.5], metadata={})\n",
                "''').strip()\n",
                "\n",
                "safe_file = TEMP_DIR / \"test_tool.py\"\n",
                "safe_file.write_text(safe_code)\n",
                "verification = verify_tool(safe_file, spec)\n",
                "check(\"Verifier: safe tool passes\", verification.passed)\n",
                "\n",
                "# Test registry\n",
                "registry = build_default_registry()\n",
                "check(\"Registry: has core tools\", len(registry) > 0)\n",
                "\n",
                "# Test graphs compile\n",
                "agent_graph = build_agent_graph()\n",
                "evo_graph = build_evolution_graph()\n",
                "check(\"Graphs compile\", agent_graph is not None and evo_graph is not None)\n",
                "\n",
                "print(\"\\nAll validation tests passed!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Live Demo: Real Event + Full Pipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import logging\n",
                "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n",
                "\n",
                "# Fetch real market\n",
                "client = KalshiClient()\n",
                "markets = client.get_active_markets(limit=10)\n",
                "print(f\"\\nFetched {len(markets)} markets:\")\n",
                "for i, m in enumerate(markets[:3]):\n",
                "    print(f\"  [{i}] {m.get('title', 'Unknown')[:50]}\")\n",
                "\n",
                "chosen = markets[0]\n",
                "event = schemas.EventInput(\n",
                "    event_id=chosen.get(\"event_id\", chosen[\"market_id\"]),\n",
                "    market_id=chosen[\"market_id\"],\n",
                "    market_title=chosen.get(\"title\", \"Unknown\"),\n",
                "    current_price=chosen.get(\"last_price\", 0.0),\n",
                "    timestamp=datetime.fromisoformat(chosen[\"timestamp\"]) if isinstance(chosen[\"timestamp\"], str) else chosen[\"timestamp\"],\n",
                ")\n",
                "print(f\"\\nSelected: {event.market_title}\")\n",
                "print(f\"Price: {event.current_price:.2f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run agent\n",
                "registry = build_default_registry()\n",
                "agent_graph = build_agent_graph()\n",
                "\n",
                "agent_result = agent_graph.invoke({\n",
                "    \"event_input\": event.model_dump(mode=\"json\"),\n",
                "    \"tools_list\": registry.list_tools(),\n",
                "    \"formula_spec\": None,\n",
                "    \"error\": None,\n",
                "})\n",
                "\n",
                "formula = schemas.FormulaSpec(**agent_result[\"formula_spec\"])\n",
                "print(f\"\\nAgent selected {len(formula.selections)} tools\")\n",
                "print(f\"Threshold: {formula.threshold:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Execute tools and score\n",
                "tool_outputs = run_tools(event, formula, registry)\n",
                "score = compute_score(tool_outputs, formula)\n",
                "print(f\"\\nScore: {score.final_score:.4f}\")\n",
                "print(f\"Bet triggered: {score.bet_triggered}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Log execution + seed borderline runs\n",
                "live_log = TEMP_DIR / \"live_log.jsonl\"\n",
                "live_result = {\n",
                "    \"run_id\": f\"live-{datetime.now(timezone.utc).strftime('%Y%m%d%H%M%S')}\",\n",
                "    \"event\": event.model_dump(mode=\"json\"),\n",
                "    \"formula\": formula.model_dump(mode=\"json\"),\n",
                "    \"score\": score.model_dump(mode=\"json\"),\n",
                "}\n",
                "log_execution(live_result, log_path=live_log)\n",
                "\n",
                "# Seed 9 borderline runs to guarantee gap detection\n",
                "for i in range(9):\n",
                "    seeded = {\n",
                "        \"run_id\": f\"seed-{i:03d}\",\n",
                "        \"event\": event.model_dump(mode=\"json\"),\n",
                "        \"formula\": {\"selections\": [{\"tool_name\": s.tool_name, \"weight\": s.weight} for s in formula.selections],\n",
                "                    \"threshold\": formula.threshold, \"rationale\": formula.rationale},\n",
                "        \"score\": {\"final_score\": formula.threshold + 0.01 * (i % 3 - 1),\n",
                "                  \"tool_outputs\": [{\"tool_name\": s.tool_name, \"output_vector\": [0.5]} for s in formula.selections],\n",
                "                  \"threshold\": formula.threshold, \"bet_triggered\": i % 2 == 0},\n",
                "    }\n",
                "    log_execution(seeded, log_path=live_log)\n",
                "\n",
                "print(f\"\\nLogged {len(live_log.read_text().strip().split(chr(10)))} entries\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze gaps\n",
                "detected_gap = analyze_gaps(log_path=live_log, min_runs=5, gap_threshold=0.05)\n",
                "if not detected_gap:\n",
                "    detected_gap = analyze_gaps(log_path=live_log, min_runs=5, gap_threshold=0.0)\n",
                "\n",
                "if detected_gap:\n",
                "    print(f\"\\nGap detected: {detected_gap.problem_detected}\")\n",
                "    print(f\"Priority: {detected_gap.priority_score:.3f}\")\n",
                "else:\n",
                "    print(\"\\nNo gap detected (unexpected)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# LLM: Generate tool spec\n",
                "if detected_gap:\n",
                "    print(\"\\nCalling LLM to generate tool spec...\")\n",
                "    proposed_spec = generate_tool_spec(detected_gap)\n",
                "    print(f\"Proposed: {proposed_spec.tool_name}\")\n",
                "    print(f\"Description: {proposed_spec.description}\")\n",
                "    print(f\"Risk: {proposed_spec.risk_level.value}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# LLM: Build tool code\n",
                "if detected_gap:\n",
                "    build_dir = TEMP_DIR / \"built\"\n",
                "    build_dir.mkdir(exist_ok=True)\n",
                "    print(f\"\\nCalling LLM to generate code for '{proposed_spec.tool_name}'...\")\n",
                "    tool_path = build_tool(proposed_spec, output_dir=build_dir)\n",
                "    print(f\"Written to: {tool_path.name}\")\n",
                "    print(f\"Size: {tool_path.stat().st_size} bytes\")\n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(\"GENERATED CODE:\")\n",
                "    print(f\"{'='*60}\")\n",
                "    print(tool_path.read_text()[:500] + \"...\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Verify tool\n",
                "if detected_gap:\n",
                "    print(f\"\\nVerifying '{proposed_spec.tool_name}'...\")\n",
                "    verification = verify_tool(tool_path, proposed_spec)\n",
                "    for check_name, passed in verification.checks.items():\n",
                "        print(f\"  {check_name}: {'PASS' if passed else 'FAIL'}\")\n",
                "    print(f\"\\nResult: {'APPROVED' if verification.passed else 'REJECTED'}\")\n",
                "    if not verification.passed:\n",
                "        print(f\"Reason: {verification.rejection_reason}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cleanup\n",
                "shutil.rmtree(TEMP_DIR, ignore_errors=True)\n",
                "print(f\"\\nCleaned up: {TEMP_DIR}\")\n",
                "print(\"\\nDONE!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}